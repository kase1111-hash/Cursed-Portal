// Auto-generated by CursedPortal AI Spec v1.0
// Source: Module M21

using UnityEngine;

/// <summary>
/// Generates and displays the AI-driven epilogue narration in OtherDimension.
/// Uses accumulated spirit memories to create a personalized farewell.
/// </summary>
public class EpilogueNarrator : MonoBehaviour
{
    [Header("References")]
    [SerializeField] private DimensionalLight spiritLight;

    [Header("Narration Settings")]
    [SerializeField] private float startDelay = 2f;
    [SerializeField] private int maxMemoryReferences = 3;

    [Header("Fallback Text")]
    [SerializeField] private string fallbackEpilogue =
        "You have traversed the veil between worlds. " +
        "The spirits whisper your name in the eternal darkness. " +
        "Remember what you have witnessed, mortal, for the echoes never truly fade...";

    // State
    private bool hasNarrated = false;

    private void Start()
    {
        if (!hasNarrated)
        {
            Invoke(nameof(BeginNarration), startDelay);
        }
    }

    /// <summary>
    /// Begins the epilogue narration sequence.
    /// </summary>
    public void BeginNarration()
    {
        if (hasNarrated) return;
        hasNarrated = true;

        Debug.Log("[EpilogueNarrator] Beginning narration...");

        // Build epilogue prompt with memory context
        string prompt = BuildEpiloguePrompt();

        // Request LLM to generate epilogue
        if (LLMManager.Instance != null)
        {
            StartCoroutine(RequestEpilogue(prompt));
        }
        else
        {
            // Fallback if LLM not available
            DisplayEpilogue(fallbackEpilogue);
        }
    }

    /// <summary>
    /// Builds the prompt for epilogue generation using spirit memories.
    /// </summary>
    private string BuildEpiloguePrompt()
    {
        System.Text.StringBuilder promptBuilder = new System.Text.StringBuilder();

        promptBuilder.AppendLine("You are the Collective Voice of all Poe spirits - the Raven, the Narrator, and Usher - merged into one.");
        promptBuilder.AppendLine("Compose a poetic farewell epilogue to the mortal who has traversed the dimensional breach.");
        promptBuilder.AppendLine("Speak in unified voice, blending themes of obsession, guilt, and decay.");
        promptBuilder.AppendLine("Keep the response under 100 words. Be haunting, poetic, and final.");
        promptBuilder.AppendLine();

        // Add memory context from all spirits
        string[] spirits = { "Raven", "Narrator", "Usher" };
        int totalMemories = 0;

        foreach (string spirit in spirits)
        {
            if (SpiritMemory.Exists(spirit))
            {
                SpiritMemory memory = SpiritMemory.Load(spirit);
                string summary = memory.GetContextSummary(maxMemoryReferences);

                if (!string.IsNullOrEmpty(summary))
                {
                    promptBuilder.AppendLine($"[{spirit}'s echoes: {summary}]");
                    totalMemories += memory.totalInteractions;
                }
            }
        }

        if (totalMemories > 0)
        {
            promptBuilder.AppendLine();
            promptBuilder.AppendLine($"Reference the {totalMemories} past hauntings in your farewell.");
        }

        // Get player name (system username as proxy)
        string playerName = System.Environment.UserName;
        if (string.IsNullOrEmpty(playerName)) playerName = "Mortal";
        promptBuilder.AppendLine($"Address the mortal as '{playerName}' once, if fitting.");

        return promptBuilder.ToString();
    }

    /// <summary>
    /// Requests epilogue from LLM.
    /// </summary>
    private System.Collections.IEnumerator RequestEpilogue(string prompt)
    {
        Debug.Log($"[EpilogueNarrator] Requesting epilogue with prompt length: {prompt.Length}");

        // Flash the spirit light
        if (spiritLight != null)
        {
            spiritLight.Flash(0.5f);
        }

        // Use LLMStreamManager for streaming response
        if (LLMStreamManager.Instance != null)
        {
            // For epilogue, we want the complete text before display
            // So we'll use a non-streaming approach

            // Fallback to direct chat
            bool responseReceived = false;
            string epilogueText = "";

            // Build request using LLMManager's configured backend
            string endpoint;
            string requestBody;
            bool useOllama = LLMManager.Instance != null
                && LLMManager.Instance.Backend == LLMManager.LLMBackend.Ollama;

            if (useOllama)
            {
                endpoint = "http://localhost:11434/api/generate";
                requestBody = JsonUtility.ToJson(new OllamaEpilogueRequest
                {
                    model = LLMManager.Instance.OllamaModel,
                    prompt = prompt + "\n\nCollective Voice:",
                    stream = false
                });
            }
            else
            {
                endpoint = "http://localhost:8080/completion";
                requestBody = JsonUtility.ToJson(new EpilogueRequest
                {
                    prompt = prompt + "\n\nCollective Voice:",
                    temperature = 0.9f,
                    n_predict = 150
                });
            }

            using (UnityEngine.Networking.UnityWebRequest request = new UnityEngine.Networking.UnityWebRequest(endpoint, "POST"))
            {
                byte[] bodyRaw = System.Text.Encoding.UTF8.GetBytes(requestBody);
                request.uploadHandler = new UnityEngine.Networking.UploadHandlerRaw(bodyRaw);
                request.downloadHandler = new UnityEngine.Networking.DownloadHandlerBuffer();
                request.SetRequestHeader("Content-Type", "application/json");
                request.timeout = 30;

                yield return request.SendWebRequest();

                if (request.result == UnityEngine.Networking.UnityWebRequest.Result.Success)
                {
                    try
                    {
                        // Try Ollama format first
                        OllamaEpilogueResponse ollamaResp = JsonUtility.FromJson<OllamaEpilogueResponse>(request.downloadHandler.text);
                        if (ollamaResp != null && !string.IsNullOrEmpty(ollamaResp.response))
                        {
                            epilogueText = ollamaResp.response;
                            responseReceived = true;
                        }
                        else
                        {
                            // Try llama.cpp format
                            EpilogueResponse resp = JsonUtility.FromJson<EpilogueResponse>(request.downloadHandler.text);
                            if (resp != null && !string.IsNullOrEmpty(resp.content))
                            {
                                epilogueText = resp.content;
                                responseReceived = true;
                            }
                        }
                    }
                    catch (System.Exception e)
                    {
                        Debug.LogWarning($"[EpilogueNarrator] Parse error: {e.Message}");
                        // Fall back to raw text
                        epilogueText = request.downloadHandler.text;
                        responseReceived = !string.IsNullOrEmpty(epilogueText);
                    }
                }
                else
                {
                    Debug.LogWarning($"[EpilogueNarrator] Request failed: {request.error}");
                }
            }

            if (responseReceived && !string.IsNullOrEmpty(epilogueText))
            {
                DisplayEpilogue(epilogueText);
            }
            else
            {
                DisplayEpilogue(fallbackEpilogue);
            }
        }
        else
        {
            DisplayEpilogue(fallbackEpilogue);
        }
    }

    /// <summary>
    /// Displays the epilogue text via UIEpilogue.
    /// </summary>
    private void DisplayEpilogue(string text)
    {
        Debug.Log($"[EpilogueNarrator] Displaying epilogue: {text.Length} chars");

        // Clean up text
        text = text.Trim();

        // Display via UIEpilogue
        if (UIEpilogue.Instance != null)
        {
            UIEpilogue.Instance.Display(text);
        }
        else
        {
            Debug.LogError("[EpilogueNarrator] UIEpilogue not found!");
        }

        // Analyze emotion and adjust lighting
        string emotion = EmotionParser.Detect(text);
        if (spiritLight != null)
        {
            spiritLight.SetEmotion(emotion);
        }
    }

    /// <summary>
    /// Force display fallback epilogue (for testing).
    /// </summary>
    public void ForceDisplayFallback()
    {
        DisplayEpilogue(fallbackEpilogue);
    }

    /// <summary>
    /// Alias for BeginNarration - called by FinaleManager.
    /// </summary>
    public void BeginEpilogue()
    {
        BeginNarration();
    }

    /// <summary>
    /// Checks if narration has begun.
    /// </summary>
    public bool HasNarrated()
    {
        return hasNarrated;
    }

    // Request/Response structures

    // llama.cpp format
    [System.Serializable]
    private class EpilogueRequest
    {
        public string prompt;
        public float temperature;
        public int n_predict;
    }

    [System.Serializable]
    private class EpilogueResponse
    {
        public string content;
    }

    // Ollama format
    [System.Serializable]
    private class OllamaEpilogueRequest
    {
        public string model;
        public string prompt;
        public bool stream;
    }

    [System.Serializable]
    private class OllamaEpilogueResponse
    {
        public string response;
        public bool done;
    }
}
