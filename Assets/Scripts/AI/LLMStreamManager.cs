// Auto-generated by CursedPortal AI Spec v1.0
// Source: Module M11

using UnityEngine;
using UnityEngine.Networking;
using System.Collections;
using System.Text;

/// <summary>
/// Handles streaming LLM responses for real-time feedback.
/// Allows spirits to "bleed" partial responses into the environment.
/// </summary>
public class LLMStreamManager : SingletonBase<LLMStreamManager>
{

    [Header("LLM Configuration")]
    [SerializeField] private string streamEndpoint = "http://localhost:11434/api/generate";
    [SerializeField] private float temperature = 0.8f;
    [SerializeField] private int maxTokens = 256;
    [SerializeField] private float requestTimeout = 60f; // Timeout in seconds

    [Header("Streaming Settings")]
    [SerializeField] private float chunkDelay = 0.05f; // Delay between chunk processing
    [SerializeField] private int minChunkSize = 5; // Minimum characters before triggering effects

    // State - use lock object for thread safety
    private readonly object streamLock = new object();
    private bool isStreaming = false;
    private Coroutine currentStreamCoroutine;
    private UnityWebRequest activeRequest; // Track active request for cancellation

    /// <summary>
    /// Starts streaming a spirit's response.
    /// </summary>
    /// <param name="systemContext">The system prompt and context</param>
    /// <param name="userMessage">The user's message</param>
    public IEnumerator StreamSpiritSpeech(string systemContext, string userMessage)
    {
        // Cancel any in-flight stream before starting a new one
        lock (streamLock)
        {
            if (isStreaming)
            {
                Debug.LogWarning("[LLMStreamManager] Already streaming, cancelling previous request");
                CancelStreamInternal();
            }
            isStreaming = true;
        }

        Debug.Log("[LLMStreamManager] Starting stream...");

        string fullPrompt = $"{systemContext}\n\nUser: {userMessage}\n\nSpirit:";

        if (UIChat.Instance != null)
        {
            UIChat.Instance.AppendSystem("*The spirit stirs...*");
        }

        // Start inner coroutine and track it under the lock
        Coroutine inner = StartCoroutine(StreamRequest(fullPrompt));
        lock (streamLock)
        {
            currentStreamCoroutine = inner;
        }

        yield return inner;

        lock (streamLock)
        {
            isStreaming = false;
            currentStreamCoroutine = null;
            activeRequest = null;
        }
        Debug.Log("[LLMStreamManager] Stream complete");
    }

    /// <summary>
    /// Performs the actual streaming HTTP request.
    /// </summary>
    private IEnumerator StreamRequest(string prompt)
    {
        // Build request body based on active backend
        string requestBody;
        bool useOllama = LLMManager.Instance != null
            && LLMManager.Instance.Backend == LLMManager.LLMBackend.Ollama;

        if (useOllama)
        {
            requestBody = JsonUtility.ToJson(new OllamaStreamRequest
            {
                model = LLMManager.Instance.OllamaModel,
                prompt = prompt,
                stream = true
            });
        }
        else
        {
            requestBody = JsonUtility.ToJson(new LLMStreamRequest
            {
                prompt = prompt,
                temperature = temperature,
                n_predict = maxTokens,
                stream = true,
                stop = new string[] { "User:", "\n\n" }
            });
        }

        using (UnityWebRequest request = new UnityWebRequest(streamEndpoint, "POST"))
        {
            // Track active request for cancellation
            activeRequest = request;

            byte[] bodyRaw = Encoding.UTF8.GetBytes(requestBody);
            request.uploadHandler = new UploadHandlerRaw(bodyRaw);
            request.downloadHandler = new DownloadHandlerBuffer();
            request.SetRequestHeader("Content-Type", "application/json");
            request.timeout = (int)requestTimeout;

            // Send request
            var operation = request.SendWebRequest();

            StringBuilder fullResponse = new StringBuilder();
            int lastProcessedLength = 0;
            float startTime = Time.time;

            // Process streaming response with timeout check
            while (!operation.isDone)
            {
                // Check for timeout
                if (Time.time - startTime > requestTimeout)
                {
                    Debug.LogError("[LLMStreamManager] Request timed out");
                    request.Abort();
                    OnStreamError("Request timed out");
                    yield break;
                }

                // Check for new data
                string currentData = request.downloadHandler.text;
                if (currentData.Length > lastProcessedLength)
                {
                    string newChunk = currentData.Substring(lastProcessedLength);
                    lastProcessedLength = currentData.Length;

                    // Process the chunk (handles multiple SSE lines)
                    yield return StartCoroutine(ProcessStreamChunk(newChunk, fullResponse));
                }

                yield return new WaitForSeconds(chunkDelay);
            }

            // Clear active request reference
            activeRequest = null;

            // Handle completion
            if (request.result == UnityWebRequest.Result.Success)
            {
                // Process any remaining data
                string finalData = request.downloadHandler.text;
                if (finalData.Length > lastProcessedLength)
                {
                    string finalChunk = finalData.Substring(lastProcessedLength);
                    yield return StartCoroutine(ProcessStreamChunk(finalChunk, fullResponse));
                }

                // Finalize response
                OnStreamComplete(fullResponse.ToString());
            }
            else
            {
                Debug.LogError($"[LLMStreamManager] Stream failed: {request.error}");
                OnStreamError(request.error);
            }
        }
    }

    /// <summary>
    /// Processes a chunk of streamed text.
    /// </summary>
    private IEnumerator ProcessStreamChunk(string chunk, StringBuilder fullResponse)
    {
        // Parse SSE format if needed (data: {...})
        string content = ParseStreamChunk(chunk);

        if (string.IsNullOrEmpty(content))
        {
            yield break;
        }

        fullResponse.Append(content);

        // Only process if chunk is meaningful
        if (content.Length >= minChunkSize || content.Contains(" ") || content.Contains("."))
        {
            // Update UI with partial response
            if (UIChat.Instance != null)
            {
                UIChat.Instance.AppendPartial(content);
            }

            // Trigger audio whisper
            if (AudioManager.Instance != null)
            {
                AudioManager.Instance.StreamWhisper(content);
            }

            // Analyze emotion and react
            string emotion = EmotionParser.Detect(content);
            if (emotion != EmotionParser.NEUTRAL)
            {
                if (EventManager.Instance != null)
                {
                    EventManager.Instance.ReactToEmotion(emotion);
                }

                if (RitualLoop.Instance != null)
                {
                    RitualLoop.Instance.ReactToEmotion(emotion);
                }
            }
        }

        yield return null;
    }

    /// <summary>
    /// Parses a stream chunk from SSE format.
    /// Handles multi-line SSE data and various LLM API formats.
    /// </summary>
    private string ParseStreamChunk(string chunk)
    {
        if (string.IsNullOrEmpty(chunk))
            return "";

        StringBuilder result = new StringBuilder();

        // Split by newlines to handle multiple SSE events in one chunk
        string[] lines = chunk.Split(new[] { "\r\n", "\r", "\n" }, System.StringSplitOptions.None);

        foreach (string line in lines)
        {
            string trimmedLine = line.Trim();

            // Skip empty lines and SSE comments/heartbeats
            if (string.IsNullOrEmpty(trimmedLine) || trimmedLine.StartsWith(":"))
                continue;

            // Skip event type declarations
            if (trimmedLine.StartsWith("event:"))
                continue;

            // Handle data lines (SSE format)
            if (trimmedLine.StartsWith("data:"))
            {
                string jsonPart = trimmedLine.Substring(5).Trim();

                // Check for stream end marker
                if (jsonPart == "[DONE]")
                    continue;

                try
                {
                    LLMStreamResponse response = JsonUtility.FromJson<LLMStreamResponse>(jsonPart);
                    if (response != null && !string.IsNullOrEmpty(response.content))
                    {
                        result.Append(response.content);
                    }
                }
                catch
                {
                    // If not valid JSON, append raw content (minus data: prefix)
                    if (!string.IsNullOrEmpty(jsonPart))
                    {
                        result.Append(jsonPart);
                    }
                }
                continue;
            }

            // Try parsing as direct JSON (non-SSE format â€” Ollama or llama.cpp)
            try
            {
                // Try Ollama format first (uses "response" field)
                OllamaStreamResponse ollamaResp = JsonUtility.FromJson<OllamaStreamResponse>(trimmedLine);
                if (ollamaResp != null && !string.IsNullOrEmpty(ollamaResp.response))
                {
                    result.Append(ollamaResp.response);
                    continue;
                }

                // Try llama.cpp format (uses "content" field)
                LLMStreamResponse response = JsonUtility.FromJson<LLMStreamResponse>(trimmedLine);
                if (response != null && !string.IsNullOrEmpty(response.content))
                {
                    result.Append(response.content);
                }
            }
            catch
            {
                // Not JSON, could be raw content - only append if meaningful
                if (!trimmedLine.StartsWith("{") && !trimmedLine.StartsWith("["))
                {
                    result.Append(trimmedLine);
                }
            }
        }

        return result.ToString();
    }

    /// <summary>
    /// Called when streaming completes successfully.
    /// </summary>
    private void OnStreamComplete(string fullResponse)
    {
        Debug.Log($"[LLMStreamManager] Full response: {fullResponse.Length} chars");

        // Final emotion analysis
        EmotionAnalysis analysis = EmotionParser.AnalyzeDetailed(fullResponse);
        Debug.Log($"[LLMStreamManager] {analysis}");

        // Increment spook based on overall intensity
        if (EventManager.Instance != null && analysis.intensity > 0.3f)
        {
            EventManager.Instance.IncrementSpook(1);
        }

        // Save to memory
        if (LLMManager.Instance != null)
        {
            string userMsg = UIChat.Instance?.GetLastUserMessage() ?? "";
            LLMManager.Instance.SaveToMemory(userMsg, fullResponse);
        }

        // Notify completion
        if (UIChat.Instance != null)
        {
            UIChat.Instance.AppendSystem("\n*The spirit's voice fades...*");
        }
    }

    /// <summary>
    /// Called when streaming encounters an error.
    /// </summary>
    private void OnStreamError(string error)
    {
        if (UIChat.Instance != null)
        {
            UIChat.Instance.AppendSystem($"*The connection shatters... ({error})*");
        }
    }

    /// <summary>
    /// Internal cancel method (called within lock).
    /// </summary>
    private void CancelStreamInternal()
    {
        if (currentStreamCoroutine != null)
        {
            StopCoroutine(currentStreamCoroutine);
            currentStreamCoroutine = null;
        }

        if (activeRequest != null)
        {
            activeRequest.Abort();
            activeRequest = null;
        }

        isStreaming = false;
    }

    /// <summary>
    /// Cancels the current stream (public API).
    /// </summary>
    public void CancelStream()
    {
        lock (streamLock)
        {
            CancelStreamInternal();
        }
        Debug.Log("[LLMStreamManager] Stream cancelled");
    }

    /// <summary>
    /// Checks if currently streaming.
    /// </summary>
    public bool IsStreaming()
    {
        return isStreaming;
    }

    // Request/Response structures

    // llama.cpp format
    [System.Serializable]
    private class LLMStreamRequest
    {
        public string prompt;
        public float temperature;
        public int n_predict;
        public bool stream;
        public string[] stop;
    }

    // Ollama format
    [System.Serializable]
    private class OllamaStreamRequest
    {
        public string model;
        public string prompt;
        public bool stream;
    }

    [System.Serializable]
    private class LLMStreamResponse
    {
        public string content;
        public bool stop;
    }

    // Ollama streams JSON lines with "response" field
    [System.Serializable]
    private class OllamaStreamResponse
    {
        public string response;
        public bool done;
    }

    protected override void OnDestroy()
    {
        // Cancel any active stream before cleanup
        CancelStream();
        base.OnDestroy();
    }
}
