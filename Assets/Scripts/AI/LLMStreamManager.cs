// Auto-generated by CursedPortal AI Spec v1.0
// Source: Module M11

using UnityEngine;
using UnityEngine.Networking;
using System.Collections;
using System.Text;

/// <summary>
/// Handles streaming LLM responses for real-time feedback.
/// Allows spirits to "bleed" partial responses into the environment.
/// </summary>
public class LLMStreamManager : MonoBehaviour
{
    public static LLMStreamManager Instance { get; private set; }

    [Header("LLM Configuration")]
    [SerializeField] private string streamEndpoint = "http://localhost:8080/completion";
    [SerializeField] private float temperature = 0.8f;
    [SerializeField] private int maxTokens = 256;

    [Header("Streaming Settings")]
    [SerializeField] private float chunkDelay = 0.05f; // Delay between chunk processing
    [SerializeField] private int minChunkSize = 5; // Minimum characters before triggering effects

    // State
    private bool isStreaming = false;
    private Coroutine currentStreamCoroutine;

    private void Awake()
    {
        // Singleton pattern with persistence
        if (Instance == null)
        {
            Instance = this;
            DontDestroyOnLoad(gameObject);
        }
        else
        {
            Destroy(gameObject);
            return;
        }
    }

    /// <summary>
    /// Starts streaming a spirit's response.
    /// </summary>
    /// <param name="systemContext">The system prompt and context</param>
    /// <param name="userMessage">The user's message</param>
    public IEnumerator StreamSpiritSpeech(string systemContext, string userMessage)
    {
        if (isStreaming)
        {
            Debug.LogWarning("[LLMStreamManager] Already streaming, ignoring request");
            yield break;
        }

        isStreaming = true;
        Debug.Log("[LLMStreamManager] Starting stream...");

        // Build full prompt
        string fullPrompt = $"{systemContext}\n\nUser: {userMessage}\n\nSpirit:";

        // Notify UI that response is starting
        if (UIChat.Instance != null)
        {
            UIChat.Instance.AppendSystem("*The spirit stirs...*");
        }

        // Use streaming request
        yield return StartCoroutine(StreamRequest(fullPrompt));

        isStreaming = false;
        Debug.Log("[LLMStreamManager] Stream complete");
    }

    /// <summary>
    /// Performs the actual streaming HTTP request.
    /// </summary>
    private IEnumerator StreamRequest(string prompt)
    {
        // Create request body
        LLMStreamRequest requestData = new LLMStreamRequest
        {
            prompt = prompt,
            temperature = temperature,
            n_predict = maxTokens,
            stream = true,
            stop = new string[] { "User:", "\n\n\n" }
        };

        string requestBody = JsonUtility.ToJson(requestData);

        using (UnityWebRequest request = new UnityWebRequest(streamEndpoint, "POST"))
        {
            byte[] bodyRaw = Encoding.UTF8.GetBytes(requestBody);
            request.uploadHandler = new UploadHandlerRaw(bodyRaw);
            request.downloadHandler = new DownloadHandlerBuffer();
            request.SetRequestHeader("Content-Type", "application/json");

            // Send request
            var operation = request.SendWebRequest();

            StringBuilder fullResponse = new StringBuilder();
            int lastProcessedLength = 0;

            // Process streaming response
            while (!operation.isDone)
            {
                // Check for new data
                string currentData = request.downloadHandler.text;
                if (currentData.Length > lastProcessedLength)
                {
                    string newChunk = currentData.Substring(lastProcessedLength);
                    lastProcessedLength = currentData.Length;

                    // Process the chunk
                    yield return StartCoroutine(ProcessStreamChunk(newChunk, fullResponse));
                }

                yield return new WaitForSeconds(chunkDelay);
            }

            // Handle completion
            if (request.result == UnityWebRequest.Result.Success)
            {
                // Process any remaining data
                string finalData = request.downloadHandler.text;
                if (finalData.Length > lastProcessedLength)
                {
                    string finalChunk = finalData.Substring(lastProcessedLength);
                    yield return StartCoroutine(ProcessStreamChunk(finalChunk, fullResponse));
                }

                // Finalize response
                OnStreamComplete(fullResponse.ToString());
            }
            else
            {
                Debug.LogError($"[LLMStreamManager] Stream failed: {request.error}");
                OnStreamError(request.error);
            }
        }
    }

    /// <summary>
    /// Processes a chunk of streamed text.
    /// </summary>
    private IEnumerator ProcessStreamChunk(string chunk, StringBuilder fullResponse)
    {
        // Parse SSE format if needed (data: {...})
        string content = ParseStreamChunk(chunk);

        if (string.IsNullOrEmpty(content))
        {
            yield break;
        }

        fullResponse.Append(content);

        // Only process if chunk is meaningful
        if (content.Length >= minChunkSize || content.Contains(" ") || content.Contains("."))
        {
            // Update UI with partial response
            if (UIChat.Instance != null)
            {
                UIChat.Instance.AppendPartial(content);
            }

            // Trigger audio whisper
            if (AudioManager.Instance != null)
            {
                AudioManager.Instance.StreamWhisper(content);
            }

            // Analyze emotion and react
            string emotion = EmotionParser.Detect(content);
            if (emotion != EmotionParser.NEUTRAL)
            {
                if (EventManager.Instance != null)
                {
                    EventManager.Instance.ReactToEmotion(emotion);
                }

                if (RitualLoop.Instance != null)
                {
                    RitualLoop.Instance.ReactToEmotion(emotion);
                }
            }
        }

        yield return null;
    }

    /// <summary>
    /// Parses a stream chunk from SSE format.
    /// </summary>
    private string ParseStreamChunk(string chunk)
    {
        // Handle different LLM API formats

        // llama.cpp format: data: {"content": "text"}
        if (chunk.StartsWith("data:"))
        {
            try
            {
                string jsonPart = chunk.Substring(5).Trim();
                if (jsonPart == "[DONE]") return "";

                LLMStreamResponse response = JsonUtility.FromJson<LLMStreamResponse>(jsonPart);
                return response.content ?? "";
            }
            catch
            {
                return chunk.Substring(5).Trim();
            }
        }

        // Direct content format
        try
        {
            LLMStreamResponse response = JsonUtility.FromJson<LLMStreamResponse>(chunk);
            return response.content ?? "";
        }
        catch
        {
            // Return raw chunk if not JSON
            return chunk;
        }
    }

    /// <summary>
    /// Called when streaming completes successfully.
    /// </summary>
    private void OnStreamComplete(string fullResponse)
    {
        Debug.Log($"[LLMStreamManager] Full response: {fullResponse.Length} chars");

        // Final emotion analysis
        EmotionAnalysis analysis = EmotionParser.AnalyzeDetailed(fullResponse);
        Debug.Log($"[LLMStreamManager] {analysis}");

        // Increment spook based on overall intensity
        if (EventManager.Instance != null && analysis.intensity > 0.3f)
        {
            EventManager.Instance.IncrementSpook(1);
        }

        // Save to memory
        if (LLMManager.Instance != null)
        {
            string userMsg = UIChat.Instance?.GetLastUserMessage() ?? "";
            LLMManager.Instance.SaveToMemory(userMsg, fullResponse);
        }

        // Notify completion
        if (UIChat.Instance != null)
        {
            UIChat.Instance.AppendSystem("\n*The spirit's voice fades...*");
        }
    }

    /// <summary>
    /// Called when streaming encounters an error.
    /// </summary>
    private void OnStreamError(string error)
    {
        if (UIChat.Instance != null)
        {
            UIChat.Instance.AppendSystem($"*The connection shatters... ({error})*");
        }
    }

    /// <summary>
    /// Cancels the current stream.
    /// </summary>
    public void CancelStream()
    {
        if (currentStreamCoroutine != null)
        {
            StopCoroutine(currentStreamCoroutine);
            currentStreamCoroutine = null;
        }
        isStreaming = false;
        Debug.Log("[LLMStreamManager] Stream cancelled");
    }

    /// <summary>
    /// Checks if currently streaming.
    /// </summary>
    public bool IsStreaming()
    {
        return isStreaming;
    }

    // Request/Response structures
    [System.Serializable]
    private class LLMStreamRequest
    {
        public string prompt;
        public float temperature;
        public int n_predict;
        public bool stream;
        public string[] stop;
    }

    [System.Serializable]
    private class LLMStreamResponse
    {
        public string content;
        public bool stop;
    }
}
